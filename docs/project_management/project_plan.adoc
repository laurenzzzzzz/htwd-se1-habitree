= Projektplan: {project-name}
Laurenz Born <laurenz.born@stud.htw-dresden.de>; Alexander Chlebowski <alexander.chlebowski@stud.htw-dresden.de>; Leonhard Hermann <leonhard.hermann@stud.htw-dresden.de>; Johann Schmidt <johann.schmidt@stud.htw-dresden.de>; Kimi Jerke <kimi.jerke@stud.htw-dresden.de>; Jannes Lehmann <s86317@htw-dresden.de>; Maurice Harnisch <s87128@htw-dresden.de>
{localdatetime}
include::../_includes/default-attributes.inc.adoc[]
// Platzhalter für weitere Dokumenten-Attribute


== Einführung
//Kurze Beschreibung und Überblick zum Dokument.


== Projektorganisation

=== Projektteamstruktur
|===
|*Name* |*Rolle*
|Alexander Chlebowski| Developer
|Jannes Lehmann| Product Owner
|Johann Schmidt| Developer
|Kimi Jerke| Developer
|Laurenz Born| Developer
|Leonhard Hermann| Scrum Master
|Maurice Harnisch| Developer

|===
// Personen nach Rollen und alphabetisch ordnen

== Praktiken und Bewertung
//Describe or reference which management and technical practices will be used in the project, such as iterative development, continuous integration, independent testing and list any changes or particular configuration to the project. Specify how you will track progress in each practice. As an example, for iterative development the team may decide to use iteration assessments and iteration burndown reports and collect metrics such as velocity (completed work item points/ iteration).

=== Managementpraktiken

|===
| *Kategorie* | *Praktik*

| Zusammenarbeit und Workflow
| Nutzung des Scrum-Frameworks zur strukturierten Projektorganisation +
- Arbeit in festen Zyklen (Sprints) von jeweils 4 Wochen +
- Weekly-Meetings (30–45 Minuten) zur Besprechung und Aufzeigung des aktuellen Arbeitsstatus der Mitglieder, Aufgabenverteilung und operativer Planung für die nächste Woche und Identifikation von Hindernissen +
- Aufgaben werden als Issues und Tasks angelegt und in einem Git-Projekt mithilfe von Eigenschaften strukturiert, um eine Gesamtübersicht sowie eine iterationsbasierte und personengebundene Darstellung zu ermöglichen +
- Aufgabenverteilung erfolgt zum großen Teil nach dem Prinzip der Freiwilligkeit zur Förderung eines positiven Arbeitsklimas in einem Team mit flachen Hierarchien, jedoch auch der Verfügbarkeit (KPI Kapazität)
| Cross Functional Team | Unser Team besteht aus Personen verschiedener Fachbereiche, die gemeinsam an unserem Projekt zusammenarbeiten. Hierfür definierten wir für unsere Mitglieder Scrum- Rollen:  +
**- Product Owner:**  +
Dieser liest sich in die Domäne ein und kommuniziert mit Stakeholdern.  +
Domänenwissen eignet er sich durch Bücher (wie z.B.: "Atomic Habits" oder "The Power of Habits") und Internet an.  +
Kommunikation mit Stakeholdern erfolgt mit dem Durchforsten von Kommentaren in Foren oder Bewertungen in Kommentarspalten bei konkurrierenden Anwendungen. Zudem wurde eine Umfrage erstellt, die mit Stakeholdern durchgeführt wurde. +
In unserem Team kümmert er maßgeblich sich um die Vision  sowie  um die User Stories.  +
**- Scrum Master:** +
Der Scrum Master organisiert und protokolliert die Meetings und hilft den Teammitgliedern bei der Konfliktbewältigung.  Dies bedeutet, dass er alle wichtigen Dinge, welche in Meetings thematisiert wurden aufnimmt und anschließend in Discord oder ins Miro- Board schreibt, Meetingräume reserviert, Schwerpunkte festlegt und ständig vor allem mit den Entwicklern potenzielle Probleme bespricht. +
Er ermöglicht ein effizientes Arbeiten innerhalb des Teams bei einer hohen Teammitgliederzufriedenheit. + 
Seine Ergebnisse schreibt der Scrum Master im  project_plan.adoc nieder.  +
Zudem unterstützt er auch den Product Owner in seinen Tätigkeiten.  +
**-Entwickler- Team: **
Das Entwickler- Team ist bei uns in der Planung und der konkreten Umsetzung der Software beteiligt. Aufgaben sind beispielsweise: +
- Erstellung/Planung der Architektur  +
- Erstellung und Planung des Domänenmodelles +
- Festlegung von User Stories (mit dem PO) und Absprache darüber welche nochmals konkretisiert werden müssen  +
- Herausarbeiten von Task aus den User Stories  +
- Programmierung eines Prototypen  +  

Sinn ist, dass jeder der Mitglieder seine Aufgaben genau kennt, versteht, dass er unersetzbar und wichtig für das Team ist und dementsprechend gemäß seiner spezialisierung Verantwortung übernimmt. Somit macht nicht jeder alles, da alle Rollen spezifische, ihnen zugeordnete Aufgaben besitzen.


| Planung & Tracking
| Sprint Planning zu Beginn jeder Iteration legt gemeinsam das Sprintziel fest +
- Bearbeitung der vom Professor bereitgestellten Iterationsdokumente +
- Zerlegung, Aufwandsschätzung, Konkretisierung und Priorisierung von User Stories und den Aufgaben in den Iterationsdokumenten. +
- Zerlegung von hochpriorisierten User Stories in kleinere Tasks, vor allem in den späteren Iterationen.  +
- Aufwandsschätzung und Zuweisung der Tasks an Teammitglieder. Task in den früheren Iterationen beziehen sich primär auf die vom Professor bereitgestellten Aufgaben aus den Iterationsdokumenten. Im Verlauf der iterativen und agilen Entwicklung liegt nun aber mehr und mehr der Fokus auch auf den User Storie- Task.  +
- Kontinuierliche Pflege des Product Backlogs (Ergänzung, Priorisierung, Schätzung, Zerlegung) durch Product Owner und Scrum Master in Abstimmung mit dem Entwicklungsteam.
| Retrospektive & Reflexion
| Nach jedem Sprint findet eine Retrospektive statt, in der das Team seine Zusammenarbeit reflektiert. +
- Jeder Mitarbeiter hat 10 min Zeit positive und negative Eindrücke und Ereignisse, die er während der letzten Iteration erfahren hat, darzulegen.  +
- Gegenseitiger Respekt ist bei uns sehr wichtig, sodass kein Mitglied während seiner Erklärungen durch andere verbessert, berichtigt oder unterbrochen werden darf.  +
- Protokollierung aller technischen, sozialen und organisatorischen Problemen im Team durch den Scrum Master. +
- Konkrete Maßnahmen zur Verbesserung werden abgeleitet und im "Lessons Learned"-Teil der Dokumentation dokumentiert.  +
- Wie alle weiteren Meetings wird die Retrospektive im Miro-Board aus Gründen der Transparenz im Team jedem sichtbar gemacht.  +
- Zum Ende der Retrospektive soll jedes Mitglied seine Hapiness bewerten. Hierfür haben wir einen Hapiness-Index eingeführt. Diesen protokollieren wir auch, um Entiwcklungen und Trends in den Emotionen einzelner Mitglieder sehen zu können.
| Review von Ergebnissen | Am Ende jeder Iteration werden unsere bisherigen Ergebnisse Stakeholdern vorgestellt. Am Ende der zweiten Iteration wurden so z.B. unser Prototyp sowie Wireframes und Wireflows einer anderen Gruppe in einem Microsofft Teams- Meeting dargelegt. Wir holten uns über diesen Weg Feedback, Verbesserungsvorschläge und Ideen für weitere User Stories und Personas ein. Zudem wurde auch ein Meeting mit unseren Coach und den Professor durchgeführt um unseren bisherigen Arbeitsstand und Arbeitsphilosophie zu bewerten. Nach jedem Meeting mit weiteren Stakeholdern besprechen wir im Anschluss noch einmal kurz die Resultate, Protokollieren diese und leiten Aufgabe/Änderungen daran ab. 
| Entwicklung | Unser Entwicklerteam arbeitet häufig zusammen und kommuniziert auch untereinander viel. Hierbei legt es individuelle Aufgaben nochmals fest und unterstützt sich gegenseitig auch bei Problemen. 
|===

=== Technische Praktiken
|===
| *Kategorie* | *Praktik*
| Ergebnisübertragung | Ziel ist eine Softwareentwicklung unter Nutzung einer versionsverwaltenden Plattform. Hierfür wird GitHub verwendet. Ergebnisse in den Dokumenten oder auch im Code werden per standardisierter Commits in das Repository übertragen. Die Vorlage für die Commits wurde im Miro- Board öffentlich jedem Mitglied zur verfügung gestellt. +
Update/Delete/Add {Beschreibungstext(Deutsch)} (Datei)
| Dokumentation 
| Projektdokumentation in Ascii Doc  +
-  Strukturierte Gliederung und Unterteilung des Projektes gemäß seiner einzelnen Komponenten(vision.adoc, project_plan.adoc, glossary.adoc etc.) +
- Gemeinsame Ablage aller Dokumente im Git Repository für eine sinnvolle und effiziente Versionierung  +
- Zusätzlich ergänzende Inhalte im Miro Board, WhatsApp-Gruppenchat und in Discord (Kommentare,  Templates, Meeting- Dokumentationen)
|UI/UX- Design
|Erstellung von Wireframes und darauf aufbauend Wireflows mit Hilfe von Figma und penpot durch das Entwicklerteam +
- Bewertung der Wireframes und Wireflows durch das gesamte Team und durch Stakeholder in Reviews. +
- Feedback wird systematisch dokumentiert, gruppiert und fließt in die Weiterentwicklung ein. 
| Fehler und Risikomangement
| Initiale Dokumentation von Risiken in separaten Git Risk Board. Hierbei werden die Risiken möglichst genau defniert.  +
- Das Risiko- Board wird bei Bedarf geupdatet und um neue Risiken ergänzt. Bestehende Risiken werden in regelmäßigen Abständen durchgangen und verwaltet hisnichtlich des Aufwandes und Einflusses.  +
- Auftretende Fehler und Risiken werden direkt in Meetings besprochen und nach Dringlichkeit priorisiert +
- Fehler und Bug-Fixes werden als eigene Tasks verwaltet und in die Iteration mit aufgenommen. (z.B.: Issues mit dem Label "Defect")
| Entwicklungstechnologie 
| Verwendung von Expo mit React Native zur plattformübergreifenden Entwicklung +
- Entwicklung in Java Script und in Type- Script  +
- Nutzung der Expo Go App zur Live Vorschau auf echter Hardware +
- Einsatz von Visual Studio Code als Entwicklungsplattform.
| Visualisierung von Aspekten unter Zuhilfenahme von Diagrammen
|- Nutzen von PlantUML um Strukturen und Interkationen innerhalb von unserem Softwaresystemen in Form von den ersten beiden Level zu einem C4 Modell zu modellieren

|===

=== Definition of Ready
[none]
Die Definition of Ready beschreibt, wann ein Backlog Item als “bereit zur Umsetzung” gilt. Dazu müssen folgende Inhalte stimmen: 

- Die Akzeptanzkriterien sind klar formuliert 
- Die Abhängigkeiten zu anderen Issues sind geklärt 
- Story ist klein genug, um in einem Sprint abgeschlossen zu werden 
- Das Team versteht die Issue/Story inhaltlich und technisch 
- Falls nötig liegt Wireframes/UI-Skizzen vor
//- Testkriterien oder Hinweise sind definiert 



=== Definition of Done

[none]
Die Definition of Done legt fest, wann ein Backlog Item als vollständig abgeschlossen gilt. Dazu müssen folgende Inhalte stimmen:

- Der Code ist implementiert und gebaut
- Der Code bzw. dessen Funktion ist in der Testumgebung funktionsfähig
- Dokumentation wurde ggf. aktualisiert
- Die Akzeptanzkriterien der Story sind erfüllt
- Die Software ist frei von kritischen/funktionseinschränkenden bekannten Fehlern

//- Product Owner hat abgenommen +
//- Es existieren automatisierte Unit Tests/Alle Tests sind erfolgreich bestanden. +

== Deployment
// Beschreibt kurz, wie und wo die Software (und deren Updates) für das Projekt bereitgestellt wird.

=== Frontend (Mobile App)

- Die App wird entwickelt und bereitgestellt für die Nutzung über die **Expo Go App**
- Der Start erfolgt durch das Scannen eines **QR-Codes** vom lokalen Entwicklungsserver
- Die Nutzung ist ausschließlich im **Netzwerk der Hochschule** (oder über eduVPN) möglich
- Diese Methode dient zur einfachen Demonstration und schnellen Iteration während des Semesters
- Es ist **keine Veröffentlichung in öffentlichen Stores** (App Store, Play Store) vorgesehen

=== Backend (API-Server)

- Das Backend wird online auf einem **bereitgestellten Windows Server der HTW** gehostet
- Die Deployment-Aktualisierungen erfolgen manuell oder automatisiert über **GitHub**
- Die API ist **öffentlich erreichbar**, solange man sich im Hoschulnetz der HTW befindet
- Der Server bleibt **bis zum Ende des Wintersemesters 2026** erreichbar

=== Datenbank

- Die **Datenbank** wird ebenfalls auf dem bereitgestellten Windows Server der HTW bis zum Projektabschluss (Wintersemester 2026) gehostet

=== Rollback und Wartung

- Da es sich um eine interne Projektumgebung handelt und keine produktive Veröffentlichung erfolgt, sind **keine komplexen Rollback-Mechanismen** notwendig
- Fehlerbehebungen erfolgen durch direkte Codeänderungen und sofortiges Testen über die Entwicklungsumgebung

=== Verfügbarkeit

- Die App ist während der Projektlaufzeit **jederzeit über den QR-Code in der Expo Go App** nutzbar
- Das Backend bleibt bis **Ende des Wintersemesters 2026** aktiv und wird danach abgeschaltet


== Diskussion zu Vorgehensweisen

== Erkenntnisse (Lessons learned)
// Führen Sie hier die wesentlichen Erkenntnisse auf, die Sie in den Retrospektiven gewonnen haben. Legen Sie besonderen Fokus auf die Maßnahmen, die Sie getroffen haben um Verbesserungen in den verschiedenen Aspekten des Projekts zu erzielen, z.B. dem Entwicklungsprozess, der technischen und organisatorischen Umgebung oder der Zusammenarbeit im Team.
=== Negative Ereignisse:
|===
|*Ereignis* |*Maßnahme* |*Erkenntnis*
|**Ausstieg aus der Gruppe von Frederic Egle (Developer)**  +
Problem: Frederic war Teil des Development-Teams, hatte jedoch bis zu seinem Austritt keine Beiträge geleistet.
| - Gruppenberatung in der ersten Retrospektive  +
- Neuverteilung von Frederics Aufgaben auf andere Teammitglieder, besonders bezogen auf das Development-Team  +
- Ist- Stand- Analyse, von Frederics bisherigem Arbeitsstand
| - Ausstieg eines Gruppenmitgliedes ist nicht unmöglich  +
- Es entsteht Mehrarbeit für die verbleidenden Gruppenmitglieder, insbesonders die Entwickler  +
- In der taktischen Planung müssen zeitliche Puffer existieren um den Erfolg des Projektes nicht zu gefährden und um in den angegebenen Zeiträumen alle Aufgaben abarbeiten zu können +
- Strategische Planung im Vorhinein und Abwägung potenzieller Risiken vor allem bei der Teamstruktur sind unabdingbar. Hierfür eignet sich eine Visualisierung im GitHub Risk- Board.
- Wöchentliche  Meetings mit kurzer Arbeitsstandangabe und nächster Schritte sind elementar um zu sehen dass jeder Mitarbeiter effizient arbeitet und das Projekt nicht gefährdet wird.
|**Terminfindung, vor allem bei außerplanmäßigen Meetings ist schwer**  | - Termine zeitig und langfristig planen  +
- Erinnerungen nochmals im Vorhinein absetzen (z.B.: über WhatsApp)  +
- Nutzen von Abstimmungen oder Ereignis-Objekten  +
- Einführung eines Team-Kalenders, z.B.: im Miro Board
| - Je größer die Gruppe ist, desto langfristiger muss die Planung erfolgen.  +
-  Nicht jeder Beteiligte muss in jedem Meeting zwangsläufig anwesend sein. So reicht es z.B.: wenn das Sprint- Review nur von 3 Personen durchgeführt wird. Hierbei ist wichtig dass die Mitarbeitenden sich abwechseln, damit jeder Mitarbeiter in einem ausgewogenen Maße entlastet werden kann, aber dennoch seine Bedeutung für die Gruppe erkennt. Dies führt dazu, dass alle Akteure sich mit der Gruppe, der Aufgabe und dem Projekt identifizieren können.  +
- Es braucht einen Organisator, der alle Meetings und Treffen sowohl innerhalb der Gruppe, als auch zwischen den Mitarbeitenden und weiteren Stakeholdern plant und organisiert. Dies ist im Scrum- Framework der Scrum Master.  +

| **Konflikte innerhalb des Teams hinsichtlich Arbeitsverteilung, Engagement und Verlässlichkeit**
| - Regelmäßige kurze Standup-Meetings, wo jeder Mitarbeitende seine Arbeitsstand, Herausforderungen und Probleme der letzten und der kommenden Woche dem Team mitteilt.  +
- Bei Konflikten ist die Kommunikation essentiell. Dabei ist es wichtig, die Konflikte bereits in Ihrer Entstehung anzusprechen und zu charakterisieren. Sonst stauen diese sich immer weiter auf und werden schwerer lösbar. Als Negativbeispiel lässt sich hier ebenfalls unser ehemaliger Entwickler Frederic anbringen.   +
|- Jeder Akteur des Teams sollte erkennen, dass er ein wichtiger und unersetzlicher Bestandteil des Teams ist, welchen Vertrauen und Wertschätzung entgegengebracht wird. Dies schafft ein Zusammenheitsgefühl, gegenseitigen Respekt, Unterstützung und Motivationssteigerung.  +
- Bewältigungen von Konflikten haben auch positive Auswirkungen, da dadurch der Gruppenzusammenhalt wächst und neue Strategien entwickelt werden können. Somit können Konflikte auch indirekt die Gruppe voran bringen.

| **Mitteilungen über unbeachteten Kommunikationskanal, was dazu führte, dass der Empfänger die Nachrichten nicht gelesen hatte und dadurch wichtige Sachen im Sprint- Review nicht dargestellt werden konnten**
|- Vereinbarung von gemeinsamen Kommunikationskanälen, die allen Teammitgliedern bekannt sind, und von jedem regelmäßig genutzt werden und genutzt werden können. (z.B.: keine Plattformabhängigkeit)  +
- Festlegung von wenigen, bestenfalls einen bis zwei festen Kommunikationswegen. In unserem Team nutzen wir WhatsApp für den täglichen Nachrichtenaustausch und GitHub für den Transfer umfangreicher Dokumente und Discord für Online-Meetings.  +
| - Vorab-Festelegung der genutzten Kommunikationsplattformen und dessen zeitlicher Nutzung.  

| **Unkenntniss über Reihenfolge des Arbeitens gemäß des Scrum- Frameworks und den Prinzipien der iterativen und agilen Entwicklung:**  +
- Verzögerungen bei der Rollenbildung  +
- Neuverteilungen von Rollen  +
- Fehlende Koordination
| - Studieren der empfohlenen Fachliteratur   +
- Nutzung aller Hinweise und fortführender Materialien (Z.B.: Anschauen des vom Professors empfohlenen YouTube-Videos )  +
- Konstanter Besuch der Vorlesungen und Praktika  +
- Nacharbeitung der Lehrveranstaltungen und des behandelten Stoffes  +
- Kontaktaufnahme bei Fragen zu Professor Anke und Herrn Zirkelbach
| - Ein Regelmäßiger Besuch der Lehrveranstaltungen schafft das Fundament und gibt Reihenfolge und Inhalt der zu erledigenden Aufgaben an.  +
- Besuch der Praktika vermittelt anhand eines kleinen Cocktailversand- Fallbeispieles grundlegendes praktisch anwandbares Wissen.  +
- Klärung von ungelösten Fragen in persönlichen Gesprächen:  +
    (1) Kleinere Fragen jeweils direkt im Anschluss an der Lehrveranstaltung  +
    (2) Größere Fragen und Anliegen nach Terminabsprache in einem längeren Gespräch
- Ein guter und regelmäßiger Kontakt zu den Betreuern und Stakeholdern ist für ein erfolgreiches Projekt unabdingbar
| Zu viele Bewertungsfelder und Kategorisierungsfelder in Git 
Hub für einzelne Issues/Boards. +
Dies steigerte die Unübersichtlichkeit innerhalb des GitHub-Projects, da zum Beispiel es für User Stories 3 Bewertungsfelder "Aufwand" gab.  +
Es war unstrukturiert, wo welche Bewertung erfolgen sollte.
| - Standardisierung aller Kategorien zur Bewertung von Labels  +
- Abstimmung im Team welche Kategorien wichtig sind und welche eher weniger.  +
- Verzicht bei Einführung neuer Labels auf unwichtige Kategorien
|- Zu viele gleichzeitige Bewertungsfelder (z.B.: Kategorien für "Priorität", "Aufwand", "Größe"...) führten zu Unübersichtlichkeit bei der Pflege, Erstellund und Konkretisierung der Issues und User Stories.   +
- Eigentliche Aussagen von Issues gingen dadurch unter. +
- Bewertung der Issues haben mehr Arbeit mit sich gebracht als Vorteile.
|===

=== Positive Ereignisse
|===
|*Ereignis* |*Erkenntnis*
| Schnelle Teambildung 
|Alle Teilnehmer außer Frederic haben sich sofort mit der Gruppe und der 
Gruppendynamik identifiziert und fühlten sich zur Gruppe zugehörig.  +
Es entstand ein freundschaftliches Verhältnis auf Augenhöhe und alle Mitglieder sind gleichberechtigt.  +
Durch die schnelle Teambildung konnten fast alle Mitglieder gleich schon von Beginn an harmonisch zusammenarbeiten ohne größere Konflikte.
| Frühzeitiger Arbeitsbeginn bei den einzelnen vorgegebenen Aufgabenblättern
| Dadurch, dass wir gleich schon von Beginn an aktiv an dem Projekt gearbeitet haben und einzelne Aufgaben auch immer in den zugehörigen Iterationen abgearbeitet haben, kam es zu weniger Stress und Zeitdruck.
| Ist- Standsanalyse und Befragung von Coach und Professor führten dazu, dass die einzelnen Mitglieder ihren eigenen Wissensstand reflektierten und aktiv sich Wissen über das agile Projektmanagement aneigneten.  +
Schlussendlich führte dies dann dazu, dass alle Beteiligten erkannten, dass die Regelungen hilfreich und sinnvoll sind und diese dann zeitnah umsetzten.
| Wenn man Probleme hat, sollte man sich nicht scheuen nachzufragen.  +
Es ist außerdem sinnvoll auf dem Wissens- und Erfahrungsschatz von Experten prinzipiell aufzubauen. 
| Erfolgreiches Einbringen eigener Ideen  zur effizienten Modifizierung 
des Projektes. Projekt  enthält eigene Gedanken und Ideen und ist somit hinsichtlich der Gestaltung und Arbeitsweise individuell.  +
- Einführung unterschiedlicher Labels für Aufgaben auf GitHub um klar zwschen den durch User Stories entstehenden Task und den vom Professor vorgegebenen Iterationsaufgaben zu differenzieren +
-  Flexible Kommunikationswege:   +
(1) GitHub für Austausch größere Dokumente und Aufgabenverteilung  +
(2) Whats- App für spontane Kommuikation und Meetingplanung  +
(3) Miro- Board für Festlegung von Zufriedenheit nach Meetings und Templates (z.B.: für Git Commit- Messages)  +
- Einführung Hapiness- Index für alle Mitarbeiter nach Retrospektiven und transparente Darstellung im Miro- Board.
| Wissen und Kenntnisse sind anwendbar und können individuell angepasst werden, sodass diese nochmals mehr den Bedürfnissen des Teams entsprechen.
|===

== Ereignisse und Praktiken in Software-Engineering 2
=== Negative Ereignisse
|===
|*Ereignis* |*Maßnahme* |*Erkenntnis*
| Inkonsistenzen zwischen PowerPoint und GitHub, vor allem bei Task und Asignees
| - Übertragung der Kapazitätenliste ins GitHub +
- Happiness-Score in Miro ausgelagert +
- PowerPoint reduziert
| - Mehrere Orte für dieselbe Information erzeugen Chaos +
- einheitliches System vereinfacht Kontrolle

| Zu hoher administrativer Aufwand durch Pflege von Power Point und Git Hub (und Miro- Board)
| - Aufgaben- und KPI-Erfassung direkt in GitHub und Miro
| - Administrative Arbeit darf nicht den Entwicklungsaufwand übersteigen
| **Anhaltend geringes Engagement eines Teammitglieds trotz wiederholter Aufgabenvergabe**
+
Bereits in Software Engineering 1 wurden vereinbarte Aufgaben (z. B. Erstellung von Wireframes) nicht oder nur unvollständig erledigt.  
In Software Engineering 2 setzte sich dieses Verhalten verstärkt fort:
+
- über mehrere Wochen keine Bearbeitung zugewiesener Aufgaben (z. B. Benutzerhandbuch, Sequenzdiagramme) +
- unregelmäßige oder fehlende Teilnahme an Meetings +
- passive Anwesenheit (stummgeschaltet, AFK, keine inhaltlichen Beiträge in Meetings) +
- fehlende Eigeninitiative und Rückmeldungen +
- Kaum Commits in Git Hub  +
- Wenn Commits vollzogen wurden, dann wurde nur Chat- Gpt Text ins Git gepushed
|
- Mehrfache freundliche Erinnerungen und persönliche Gespräche im Team, vor allem veranlasst durch Scrum Master, was keine Wirkung erzielte +  
- Klare Aufgabenverteilung mit festen Zuständigkeiten und Deadlines, was auch kaum Wirkung erzielte  +
- Thematisierung des Problems in Retrospektiven, was ebenfalls wirkungslos war + 
- Eskalation auf eine deutlich verbindlichere, autoritärere Kommunikation, was schlussendlich Erfolge in der Arbeitsweise der Person mit sich brachte:
  +
  - explizite Delegation von Aufgaben +
  - klare Erwartungshaltung mit Konsequenzen, wenn diese nicht erfüllt wurde +
  - Hinweis auf mögliche Einbindung von Professor/Coach bzw. Ausschluss aus dem Team bei weiterem Ausbleiben von Beiträgen +
|
- Freundliche Ermahnungen allein reichen nicht aus, wenn grundlegende Verlässlichkeit fehlt  +
- Scrum setzt Selbstorganisation voraus, benötigt aber **klare Grenzen und Verbindlichkeit**, wenn diese nicht eingehalten wird  +
- Autoritäres Eingreifen ist kein Widerspruch zu Agilität, sondern in Ausnahmesituationen notwendig, um das Projekt zu schützen (Beschrieben auch z.B.: im Situativen Reifegradmodell der Führung nach Hersey/Blanchard)  +
- Fehlende Beiträge einzelner gefährden nicht nur Ergebnisse, sondern auch kaskadierend Motivation und Fairness im Team  +
- Frühes und konsequentes Handeln ist entscheidend, damit sich die unerledigten Task der Person nicht aufsummieren +
- Man sollte sich nicht einlullen lassen und nicht immer Ausreden wie 'Das werde ich heute Abend noch machen...' akzeotieren. +
| Ungeeignete Sprintlänge und ungenaue Sprintplanung im universitären Kontext
| - Einführung eines Kapazitätenboards zur Verfassung verfügbarer Arbeitszeit +
- Wenn innerhalb eines Sprints noch mehr Kapazität besteht und bereits alle Artefakte abgearbeitet wurden, werden in einem Meeting mit dem gesamten Team neue Artefakte in den Sprint aufgenommen. +
- Wenn User Stories od. andere Artefakte nicht abgeschlossen werden können innerhalb eines Sprints, werden sie erst einmal in den Product Backlog zurückgeführt +
- Ggf. werden Stories die nicht abgearbeitet konnten in feinere Einheiten zerlegt am Sprintende (innerhalb von Refinement Meetings), allenfalls wird deren Komplexität aber vom Team erneut kritisch überprüft. +
| - Eine feste Sprintlänge von vier Wochen ist im Studium nur eingeschränkt planbar, da Arbeitsbelastung (u.a. durch andere Module oder dem Projektseminar aber auch durch außeruniversitäre Belastungen einzelner Mitglieder) stark schwankt +
- Kapazitäten lassen sich im Studium nur bedingt und kurzfristig planen +
- Sowohl die Über als auch die Unterplanung eines Sprints wirken sich negativ auf die Effizienz innerhalb des Teams aus, da immer Abstimmungs oder zusätzlicher Klärungsbedarf besteht. +
- Sprintziele müssen selbstverständlich realistisch und flexibel geplant werden +
- Mittels eines stabilen Product Backlogs kann man auch kleinere Sprintschwankungen während des Semestersgut austahieren. +
- Ferner ist für studentische Projekte eine Adaptivität teilweise wichtiger als eine strikte Methodentreue.
|===

=== Positive Ereignisse
|===
|*Ereignis* |*Erkenntnis* 
| Einführung klarer KPIs (Hapiness, Kapazität, Task- finish rate)
| - Sichtbarkeit der Arbeitsbelastung und Motivation verbessert Planung, Teamverständnis, Zusammenarbeit, und dienem
dem Scrum Master als Arbeitsunterstützung (z.B.: Ansprechen in Retrospektive, wenn Hapiness stark fällt, oder sich Kapazitäten drastisch reduzieren)
| Meetings bekommen prinzipiell durch Power Point mehr Struktur
| - Vereinheitlichung der Besprechungsbasis führt dazu, das alle Teammitglieder den Stand, Ziele und mögliche Blockaden kennen
- Vereinfachung der Koordination innerhalb des Teams durch gleiches Projektverständnis
| Klare Entscheidung gegen PowerPoint als Hauptwerkzeug, bei gleichzeitigem Aufrechterhalten der verbesserten Meeting Struktur
| - Weniger Redundanz, weniger Aufwand, konsistentere Planung
| Einführung strukturierter Testprozesse entlang der Testpyramide
|- Grundlegende Ziele des Testens ist das extrahieren von: +
(1) Fehlerhaften Zuständen des Systemes +
(2) Ausfällen, Abweichungen des Ist- Verhaltens vom Soll- Verhalten des Systemes +
(3) Bugs/ `Defects`, mechanische oder algorithmische Fehler +
- Aufbau eines systematischen Testprozesses entlang der in der Vorlesung vorgegebenen Testpyramide +
- Akzeptanztests wurden im Weekly ausgewählt, priorisiert und im Rahmen der Sprint Reviews gemeinsam mit Stakeholdern überprüft (Team 1C) +
- Alle während des Testens erkannten Fehler werden als Bugs dokumentiert und mit dem Label `defect` erfasst +
- `Defects` werden automatisiert auf einem separaten Board angezeigt und im Weekly den verantwortlichen  zugeteilt +
- Zusätzliche Fehler, die während des Programmierens auftreten, werden durch den jeweiligen Entwickler bearbeitet +

| Durchführung eines strukturierten Sprint- Review mit einem externen Team (Team 1c), welches zentrale Funktionalitäten der Habitree- App getestet und bewertet hat. +
| -	Externes Feedback liefert wertvolle Perspektiven, die innerhalb des eigenen Teams oft übersehen werden (z. B. Notwendigkeit von Features, Gestaltungsvorschläge...). +
	-	Die Kombination aus Abnahmetest und Feedback (UI- Gestaltung, Feature- Funktionen...) gibt uns wertvollen Input um unser Produkt weiter verbessern zu könenn +
	-	Ein klar strukturiertes Sprint-Review (Agenda, Zeitrahmen, Zielsetzung) schafft effizientes und Zielorientiertes Arbeiten. (Statt sich über andere Dinge den Großteil der Zeit zu unterhalten wie in SE1) +
	-	Das Review bestätigte sowohl funktionale Stärken als auch  Verbesserungspotenziale für die nächsten Sprints +
|===

=== Managementtechniken
|===
|*Kategorie* |*Praktik* 
| Meeting-Struktur- Verbesserung
|- Wöchentliche Weeklies +
- Retrospektive am Ende des Sprint +
- Happiness-Erhebung zu Retrospektiven +
- Kapazitätserhebung im Weekly +
- Sprint Planning Meeting zu Beginn jedes Sprints +
- Refinement- Meeting wenn PBI's zu komplex sind und nicht innerhalb eines Sprints trotz ursprünglichen Erwartungen umgesetzt werden konnten
| KPI-Nutzung
|- Happiness-Index (avg(Hapiness- Board)) +
- Kapazitäten-Index (avg(Kapazitäten-Board)) +
- Task-Fortschrittsquote (erledigte Task / Gesamttask) +
| Sprint- Management
| - Einführung echter 4-Wochen- Sprints, Sprint Backlog, Sprint Execution und regelmäßigen Reviews
| Task- Organisation
| - Festlegung klarer Task- Typen: +
(1) Documentation- Task +
(2) Implementation- Task +
(3) Story- Task
| User- Story- Vorbereitung
| - Ergänzung des Git Hub Boardes um ein `ready`- Label, welches nur User Stories- Erhalten, die alle Kriterien unserer
Definition of Ready enthalten +
| Git- Hub- Nutzung
| - Board vollständig restrukturiert: +
(1) Herausnahme der Komplexität des Boardes durch Entfernen von redundanten Sichten und Labels +
(2) Ergänzen/Refactoring des Boardes um neue, sinnstiftende Sichten und Labels, wie unter anderem: +
- Product Backlog: Unsere User Stories (gruppiert nach Priorität nach Moscow) +
- Sprint Backlog: Abgeleitete Task der User Stories +
- Sprint Execution: Aktuelle User Stories, des jeweiligen Sprints; Board gruppiert nach den einzelnen Sprints, sodass eine gewisse Zeitreihe entsteht; Aktueller Sprint wird mit dem Label `current` visualisiert  +
- Current Iteration: Zeigt alle Task {Story-, Documentation,- Implementation- Task} der jeweiligen Iteration, gefiltert nach `ToDo`,`In Progress`, `Done` +
- Documentation & Review- Task: Zeigt alle Dokumentationsanforderungen sowie Termine/ Verbindlichkeiten für den Stakeholder- Austausch +
(3) Standard- Workflows für Labelvergabe und Umgang mit Sprints definiert, unter anderem: +
- Unterteilung der Task nun in  `Implementation`, `Story`, `Documentation`- Task zur besseren Struktur und Übersichtlichkeit +
- Herausnahme von redundanten Komplexitäts und Prioritätslabeln, sodass ein einheitliches Verständnis über den Sinn und Nutzen der Labels nun vorliegt und die Labelingstrategie bei allen Artefakten konsistent und gleich erfolgt +
- Wiederaufnahme von Sprints und Iterations- Labels in Git- Hub um nun ganz genau einblicken zu können welche Task und schlussendlich auch welche User Stories zu welchen Sprints finalisiert wurden +
(4) User Stories- Format vereinheitlicht +
| Umgang mit geringer Verlässlichkeit im Team
| - Klare Erwartungshaltung an jedes Teammitglied hinsichtlich Teilnahme, Kommunikation und Aufgabenerfüllung  +
- *Eskalationsstufen bei anhaltender Nichterfüllung:* +
  (1) freundliche Erinnerung und Unterstützung  +
  (2) explizite Aufgaben und Terminzuweisung  +
  (3) autoritäre Delegation der Aufgaben, mit Androhung von Konsequenzen bei Nichteinhaltung, *auch wenn es sich mit manchen Scrum- Prinzipien beißt!*  +
- Finden einer Balance zwischen Projekterfolg und Teamspirit +
- Schutz des Teams vor dauerhafter Mehrbelastung durch einzelne Mitglieder ist wichtig um andere Mitglieder nicht auch zu demotivieren!  +
- Einbindung von Betreuern (Professor/Coach) als letzte Eskalationsstufe +
| Sprint Management in Abhängigkeit von äußeren Umständen
| - Einführung eines differenzierten Sprint Managements um mit stark schwankenden Kapazitäten (bedingt durch Stress basierend auf anderen Modulen/ Seminaren oder auch außeruniversitären Belastungen) klar zu kommen. +
- Es müssen hierbei 2 Zustände berücksichtigt werden: +
*(1) Teammitglieder sind mit den einzelnen Sprintaufgaben deutlich vor Sprintende fertig:* +
- Deutlich vor Sprintende  bedeutet über 10 Tage Leerlaufzeit +
- Durchführung eines `Sprint- Refill- Meetings` +
*- Ziel:*  Vermeidung ungenutzer Leerlaufzeit und Sicherstellung kontinuierlicher Wertlieferung und Anwendungs- Verbesserung +
*- Grund:* Studenten haben nicht die Zeit, solche wertvollen Leerlaufzeiten ungenutzt zu lassen, sondern müssen so effizient wie möglich Features releasen. (v.a. da Stress gerade zu Ende des Semesters allgemein stark ansteigt) +
- Aufnahme von zusätzlichen Stories aus dem Product Backlog in den laufenden Sprint, inklusive kurzer Neubewertung von Komplexität, Risiken und Abhängigkeiten zu nachgelagerten Inhalten und Taskableitung. +
*(2) Nicht alle Artefakte können vor dem definierten Sprintende abgeschlossen werden:* +
- Herausnahme der Stories aus dem Sprint +
- Ansprechen in der Retrospektive und Ermittlung der Gründe für die Fehlende Abarbeitung +
- Erneutes Refinement- Meeting und ggf. Überarbeitung der Artefakte (nur falls diese wirklich zu komplex sein sollten; häufig ist schlichtweg Stress, induziert durch andere Fächer und Seminare der Universität Schuld an der fehlenden Abarbeitung) +
- Wiedereingliederung der alten Stories in den nächsten Sprint, noch vor den neuen Stories +
*-Anmerkung:* Dieses Vorgehen weicht von klassischen Scrum- Prinzipien ab, vor allem da man nicht einfach einen festgelegten Sprint mit neuen Inhalten befüllen sollte, wenn man all seine Task schon vor der eigentlichen Zeit abgearbeitet und validiert hat. +
Wir haben uns dazu entschieden, dass dennoch so auszuleben, da das Zeitfenster für dieses Projekt allgemein sehr klein ist und wir uns die entstehende Leerlaufzeit nicht in dem Umfang leisten können. +
Grund hierfür ist das universitäre Umfeld in dem sich die Projektteilnehmer befinden. Dies zeichnet sich aus durch unregelmäßige Kapazitäten, Prüfungsphasen und Deadlines in anderen Modulen oder dem Projektseminar, die teilweiße kurzfristig und unerwartet aufkommen.

|===


=== Technische Praktiken

|===
|*Kategorie* | *Praktik*

| Test-Management & Qualitätssicherung
|*Unsere Tests orientieren sich an den FIRST-Prinzipien*: +
(1) *Fast*: Tests laufen schnell, keine echten API- oder Netzwerkaufrufe +
(2) *Independent*: Tests sind unabhängig voneinander (`beforeEach` / Reset des Zustands) +
(3) *Repeatable*: Tests sind reproduzierbar, keine externen Abhängigkeiten +
(4) *Self-Validating*: Tests enthalten klare Assertions +
(5) *Timely*: Tests werden begleitend zur Implementierung erstellt +
- Habitree wird sowohl *verifiziert* (funktionale Korrektheit) als auch *validiert* (Erfüllung der User Stories). +

*Unsere Tests untergliedern sich in mehrere Arten:* +
(1) *White-Box-Tests*: +
- Messung mittels der KPI `Code Coverage` +
- Ziel: Prüfung interner Logik, Kontrollflüsse und Randfälle  auf Basis der Implementierung +
(2) *Black-Box-Tests*: +
- Prüfung des Systemverhaltens aus Nutzersicht anhand von User Stories und Akzeptanzkriterien +
- Der interne Code wird dabei als unbekannt betrachtet
- Diese Tests erfolgen sowohl explorativ (manuelles Durchklicken) als auch strukturiert +

*Methoden der Tests:* +
(1) Äquivalenzklassenanalyse  +
(2) Grenzwertanalyse +

*Namenskonventionen:* +
- standardisiert: +
`<Funktion>_<Szenario>_<ErwartetesErgebnis>` +

*Ordnerstruktur:* +
- Vorhanden und dokumentiert im Dokument `testing.adoc` +
- Tests werden fortlaufend während des Sprints erstellt und spätestens zur Prüfung der *Definition of Done* abgeschlossen. +
- Eine User Story darf nur dann auf *Done* gesetzt werden, wenn alle relevanten Tests erfolgreich sind. +

| Unterteilung der Testarten
|*- Einführung einer Testpyramide mit klaren Teststufen:* +
  (1) Unit-Tests zur Überprüfung einzelner Methoden (je Test i.d.R. eine Methode) +
  (2) Integrationstest um die Zusammenarbeit mehrerer Units zu testen.  +
  (3) Systemtests für das gesamte System. +
  *- Testfälle werden in Positiv und Negativtests untergliedert:* +
  (1) Positivtests: Das Verhaltens des System under Tests bei korrekter Nutzung wird untersucht, was bedeutet, dass nur valider Input als Testdaten eingefügt wird. +
  (2) Negativtests: Das Verhalten des System under Tests bei fehlerhaften/unzureichenden Input wird überprüft, sodass insbesonders dessen Exception- Handling ersichtlich wird. 
  
| Integration ins GitHub-Board
|- Bugs, die durch Tests gefunden werden, werden als Issues mit dem label `defect` versehen +
- Defects erscheinen automatisch im separaten Defect-Board +
- Defects werden basierend auf Ihrem Risiko und Ihrer Komplexität geschätzt und priorisiert. Für die Priorisierung wird auch der Geschäftswert der betreffenden Funktionalität betrachtet. Ist dieser hoch, so muss das Fixing zeitnah erfolgen!  +
- Bugs, die von den einzelnen Entwicklern selber im jeweiligen Sprint gefunden wurden, werden sofort durch die zuständigen Programmierer behoben. +


| Integration in den Scrum- Zyklus
|- Sprint Reviews enthalten nun auch Akzeptanztests zu vorher ausgewählten Stories/ Features der Anwendung. Somit sollen auch statische Tests manuell durch andere Personen als den Ersteller abgedeckt werden. + 
- Weitere statische Tests werden auch innerhalb der Weekly- Meetings durch andere Personen (insbesonders durch unseren PO) des Teams vorgenommen. +
- Vor allem bei Sprint Review- Tests mit externen Stakeholdern,erfolgt im Vorfeld eine kurze Absprache mit dem Scrum Master, PO und dem enstprechenden Entwickler, welche konkreten Kriterien zur Testevaluation hinzugezogen werden müssen. +
|===




| Test-Framework & Werkzeuge
a|*Framework:* +
- Wir nutzen Jest als Test-Framework mit dem `jest-expo` Preset, damit alles mit React Native und Expo zusammenpasst +
- Die genaue Konfiguration steht in unserer `jest.config.js`, wo wir auch bestimmte React Native Module von der Transformation ausschließen mussten +

*Mocking:* +
- Für unsere Tests mocken wir die Repository-Interfaces mit Jest Mock Functions (`jest.fn()`) +
- Wir haben eigene Mock-Implementierungen geschrieben (z.B. `MockHabitsRepository` in den Integrationstests), damit wir keine echten API-Calls machen müssen +
- So stellen wir sicher, dass unsere Tests wirklich *Independent* laufen (FIRST-Prinzip) +

*Code Coverage:* +
- Jest misst automatisch unsere Code Coverage für die `domain/**` und `application/**` Layer +
- Wir haben uns Coverage-Schwellwerte gesetzt: +
  * Branches: 60% +
  * Functions: 60% +
  * Lines: 60% +
  * Statements: 60% +
- Mit `npm run test:coverage` können wir uns den Coverage-Report anzeigen lassen +

| Automatisierung & CI/CD
a|*GitHub Actions Workflow:* +
- Unsere Tests laufen automatisch bei jedem Push auf den `main` Branch (wenn was in `src/**` geändert wurde) +
- Der Workflow in `.github/workflows/main.yml` läuft so ab: +
  (1) Code auschecken +
  (2) Node.js (Version 20.x) installieren +
  (3) Dependencies installieren mit `npm ci` +
  (4) Security Scan mit Shai-Hulud Detector durchführen +
  (5) Expo Doctor für Projekt-Check laufen lassen +
  (6) Alle Tests mit `npm test` ausführen +
- Wenn Tests fehlschlagen, bricht der ganze Build ab und wir sehen das direkt in GitHub +

*Lokale Entwicklung:* +
- `npm test`: Führt alle Tests einmal aus +
- `npm run test:watch`: Tests laufen kontinuierlich im Hintergrund während wir entwickeln +
- `npm run test:coverage`: Zeigt uns den Coverage-Report an +

| Testorganisation & Struktur
a|*Ordnerstruktur:* +
Wir haben unsere Tests im `__tests__/` Ordner nach Testtypen organisiert: +
[source]
----
__tests__/
├── domain/              # Unit Tests für Entities
│   ├── Habit.entity.test.ts
│   ├── User.test.ts
│   └── Quote.test.ts
├── integration/         # Integrationstests für Services
│   ├── HabitService.integration.test.ts
│   ├── QuoteService.test.ts
│   ├── AuthService.test.ts
│   ├── ProfileService.test.ts
│   └── HabitSchedulePolicy.test.ts
└── system/             # Manuelle Systemtests
    └── SYSTEMTESTS.md
----

*Namenskonventionen:* +
- Alle Testdateien enden auf `.test.ts` oder `.test.tsx` +
- Jede Datei testet genau eine Entity oder einen Service (z.B. `User.test.ts` testet nur `User.ts`) +

*Aufbau einer Testdatei:* +
- `describe()` Blöcke fassen Tests für eine Funktion/Methode zusammen +
- `beforeEach()` setzt vor jedem Test alles zurück (Mocks, State), damit die Tests unabhängig sind +
- `it()` bzw. `test()` schreibt den einzelnen Testfall +

| Konkrete Umsetzung White-Box-Tests
a|*Wie wir White-Box testen:* +
- Jest misst automatisch die Coverage und zeigt uns welche Code-Zeilen noch nicht getestet sind +
- Wir konzentrieren uns auf Domain- und Application-Layer (UI und Infrastructure testen wir anders) +
- Ziel ist es, alle wichtigen Kontrollflüsse und if/else-Verzweigungen abzudecken +

*Konkrete Beispiele aus unserem Code:* +
- In `Habit.entity.test.ts` testen wir z.B. die `getStreak()` Funktion mit: +
  * Leerem Array (was passiert wenn keine Entries da sind?) +
  * Genau einem Entry (Grenzfall) +
  * Mehreren aufeinanderfolgenden Tagen (Normalfall) +
  * Lücke mittendrin (Streak muss abbrechen) +
  * Entry mit `status: false` (Streak-Unterbrechung) +
  * Unsortierten Entries (testet die Sortier-Logik) +
- Bei `User.test.ts` für die Email-Validierung testen wir: +
  * Gültige Emails wie `test@example.com` (Positivtests) +
  * Fehlende @, fehlende Domain, fehlende TLD (alle Negativtest-Branches) +
  * Leerzeichen und leere Strings (Edge Cases) +

*Unsere Testdaten-Strategie:* +
- Äquivalenzklassen bilden: Gültige Eingaben vs. ungültige Eingaben trennen +
- Grenzwerte testen: z.B. Username mit 3 Zeichen (Minimum), 50 Zeichen (Maximum), leer (ungültig) +

| Manuelle & Explorative Tests
a|*Systemtests (Manuell):* +
- Haben wir in `__tests__/system/SYSTEMTESTS.md` dokumentiert +
- Machen wir vor jedem Release - dauert ca. 5 Minuten durchzuklicken +
- Unsere Testfälle sind: +
  * ST-01: Login-Flow durchgehen +
  * ST-02: Habit anlegen, bearbeiten, löschen (CRUD) +
  * ST-03: Habit mehrere Tage abhaken und Streak checken +
  * ST-04: Validierungen testen (z.B. leere Felder) +
- Wenn wir dabei Bugs finden, legen wir sofort ein GitHub Issue mit Label `defect` an +

*Explorative Tests:* +
- Machen wir in unseren Weekly-Meetings zusammen mit dem PO +
- Einfach mal neue Features durchklicken ohne festen Plan +
- Fokus liegt auf Usability und unerwarteten Fehlern die uns auffallen +

*Akzeptanztests:* +
- Haben wir in Sprint Reviews gemacht, z.B. mit Team 1C +
- Die haben unsere App getestet und geschaut ob die User Stories erfüllt sind +
- Deren Feedback und gefundene Fehler haben wir dokumentiert und als neue Tasks angelegt +

| Konkrete Beispiele zu FIRST-Prinzipien
a|Wir achten bei unseren Tests auf die FIRST-Prinzipien: +

*Fast (Schnell):* +
- Unsere Unit Tests laufen alle in unter 2 Sekunden durch +
- Beispiel: In `HabitService.integration.test.ts` nutzen wir `MockHabitsRepository` statt echtem Server, deshalb geht's schnell +

*Independent (Unabhängig):* +
- Jeder Test kann alleine laufen, ohne dass vorher andere Tests durchgelaufen sein müssen +
- Beispiel: In `QuoteService.test.ts` (Zeile 9-15) initialisieren wir in `beforeEach()` immer ein frisches Mock-Repository +

*Repeatable (Wiederholbar):* +
- Tests liefern immer das gleiche Ergebnis, egal wann und wo wir sie laufen lassen +
- Beispiel: `AuthService.test.ts` mockt alle Repository-Calls, damit nichts von externer API oder Datenbank abhängt +

*Self-Validating (Selbstvalidierend):* +
- Jeder Test sagt klar "Pass" oder "Fail", ohne dass wir was manuell prüfen müssen +
- Beispiel: `Quote.test.ts:83` hat klare Assertion → `expect(quote.isValid()).toBe(false)` +

*Timely (Rechtzeitig):* +
- Wir schreiben Tests parallel zur Implementierung, nicht erst später +
- User Stories setzen wir nur auf *Done*, wenn die Tests grün sind +

| Welche Qualitätseigenschaften wir am meisten testen
a|Wir orientieren uns am ISO 25010 Software-Qualitätsmodell und fokussieren uns besonders auf: +

(1) *Funktionale Korrektheit:* +
- Wir testen ob unsere Business-Logik richtig funktioniert (z.B. Streak-Berechnung, Email-Validierung) +
- Positivtests: Funktioniert es mit korrekten Eingaben? +
- Negativtests: Werden Fehler richtig behandelt? +

(2) *Zuverlässigkeit:* +
- Testen ob die App auch bei Fehlern stabil bleibt +
- Beispiel: `AuthService.test.ts:81-86` testet was passiert wenn jemand falsche Login-Daten eingibt +

(3) *Wartbarkeit:* +
- Durch unsere Layer-Architektur (Domain, Application, Infrastructure) können wir einzelne Teile gut testen +
- Dependency Injection macht's einfach: Services bekommen Repositories als Parameter, die wir dann einfach mocken können +

(4) *Portabilität:* +
- Unsere Tests laufen überall: Linux, macOS, Windows +
- Keine Abhängigkeiten von spezieller Hardware oder Betriebssystem-Features +